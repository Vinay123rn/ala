{"cells":[{"cell_type":"markdown","metadata":{"id":"M7g7bxFCHxGP"},"source":["$${\\color{yellow}{\\text{Applied Linear Algebra: Vectors and Matrices}}}$$\n","\n"]},{"cell_type":"markdown","source":["---\n","\n","Restart the session after executing the following cell\n","\n","---"],"metadata":{"id":"H6RvcDldxHPO"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"LapV0XR-wxSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install yfinance --quiet"],"metadata":{"id":"2Gc9U_HRv-OQ","executionInfo":{"status":"ok","timestamp":1756543787067,"user_tz":-330,"elapsed":6318,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Load essential libraries\n","\n","---"],"metadata":{"id":"jDK9fC6uiBGE"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"20W0d4ruQjE4","executionInfo":{"status":"ok","timestamp":1756543813553,"user_tz":-330,"elapsed":22668,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import sys\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n","import nltk\n","import gensim.downloader\n","from nltk.tokenize import word_tokenize\n","import yfinance as yf\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"sfYXkqmLiVLM"},"source":["---\n","\n","Mount Google Drive folder if running Google Colab\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYzBBBxqiaGa"},"outputs":[],"source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE_vinay/ALA'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"]},{"cell_type":"markdown","metadata":{"id":"avVZ6D1ZgEUT"},"source":["---\n","\n","**We will now use Pytorch to create tensors**\n","\n","The patient data matrix:\n","\n","![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=1000)\n","\n","**Notation**:\n","\n","Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zrPnepAEvr0O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756543823384,"user_tz":-330,"elapsed":141,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}},"outputId":"ba584642-f1bd-4f2f-c932-032eb140fdcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]])\n","torch.Size([6, 5])\n","<class 'torch.Tensor'>\n","tensor([ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000])\n","tensor([ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000])\n","------------\n","tensor(37.3000)\n","tensor([37.3000, 37.0000, 38.5000, 38.0000, 38.3000, 37.2000])\n"]}],"source":["## Create a patient data matrix as a constant tensor\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                  [85, 130, 37.0, 110, 14],\n","                  [68, 110, 38.5, 125, 34],\n","                  [90, 140, 38.0, 130, 26],\n","                  [84, 132, 38.3, 146, 30],\n","                  [78, 128, 37.2, 102, 12]])\n","print(X)\n","print(X.shape)\n","print(type(X))\n","print(X[0]) # this is patient-0 information which is a rank-1 tensor\n","print(X[0, :]) # patient-0 all features\n","print('------------')\n","print(X[0, 2]) # feature-2 of patient-0, temperature of patient-0\n","print(X[:, 2]) # feature-2 of all patients, temperature of all patients"]},{"cell_type":"markdown","metadata":{"id":"cevtn_b4gek5"},"source":["---\n","\n","**Convert a PyTorch object into a numpy array**\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrYQ2moygfPu"},"outputs":[],"source":["print(X.numpy())\n","print(type(X.numpy()))"]},{"cell_type":"markdown","metadata":{"id":"QS3MmzwsgkWU"},"source":["---\n","\n","**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n","\n","![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n","\n","![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n","\n","![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n","\n","![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n","\n","![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgPtJP0sglQP"},"outputs":[],"source":["# Vector addition\n","print(X[1, :] + X[2, :])\n","\n","# Vector subtraction\n","print(X[1, :] - X[2, :])\n","\n","# Scalar-vector multiplication\n","print(X[:, 2])\n","print((9/5)*X[:, 2]+32) # 0peration not defined in pen & paper but in computation is referred to as\n","# broadcasting\n","\n","# Average patient\n","x_avg = (1/6)*(X[0, :] + X[1, :] + X[2, :] + X[3, :] + X[4, :] + X[5, :])\n","x_avg = torch.mean(X, dim = 0) # dim = 0 means top-to-bottom or along dim-0\n","\n","# Another broadcasting example\n","print(X)\n","print(x_avg)\n","print(X - x_avg)"]},{"cell_type":"markdown","metadata":{"id":"1t_qXrlCROKA"},"source":["---\n","\n","Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_e13FnW0RUwy"},"outputs":[],"source":["model = gensim.downloader.load('glove-wiki-gigaword-50')"]},{"cell_type":"markdown","metadata":{"id":"7YRVJferRlK5"},"source":["---\n","\n","Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n","\n","Next, we will do an *intuitive* subtraction of word embeddings as in\n","\n","1. Cricket without Tendulkar\n","2. Football without Messi\n","\n","Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVVFzeQyR3Wb"},"outputs":[],"source":["# Cricket without Tendulkar\n","a = model['cricket'] - model['tendulkar']\n","\n","# Football without Messi\n","b = model['football'] - model['messi']\n","print(a)\n","print(b)\n","\n","# How different is cricket-without-tendulkar from\n","# football-without-messi?\n","print(a-b)"]},{"cell_type":"markdown","source":["---\n","\n","Understanding pen & paper versions of tensors w.r.t. their representations in the code\n","\n","---"],"metadata":{"id":"O6nbdX9IAYu6"}},{"cell_type":"code","source":["# Pen & paper: 3-vector, Code: rank-1 tensor\n","a_vector = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n","print(a_vector)\n","print(a_vector.shape)\n","print('-------')\n","# Pen & paper: 1x3-matrix, Code: rank-2 tensor\n","a_matrix_version1 = torch.tensor([[1.0, 2.0, 3.0]], dtype = torch.float64)\n","print(a_matrix_version1)\n","print(a_matrix_version1.shape)\n","# Pen & paper: 3x1-matrix, Code: rank-2 tensor\n","a_matrix_version2 = torch.tensor([[1.0], [2.0], [3.0]], dtype = torch.float64)\n","print(a_matrix_version2)\n","print(a_matrix_version2.shape)"],"metadata":{"id":"WhYNdr8DAj2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VPICS8ggvvg"},"source":["---\n","\n","A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP). Assume that admission time is 9AM.\n","\n","---"]},{"cell_type":"code","source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# dim-0 as 4 hourly timestamps,\n","# dim-1 as 3 patients, and\n","# dim-2 as 2 features (HR and BP)\n","# T = torch.tensor([[[HR, BP], [HR, BP], [HR, BP]],\n","#                   [[HR, BP], [HR, BP], [HR, BP]],\n","#                   [[HR, BP], [HR, BP], [HR, BP]],\n","#                   [[HR, BP], [HR, BP], [HR, BP]]])\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)"],"metadata":{"id":"yQAvgkRkWAM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JV0fpSojg2EZ"},"source":["---\n","\n","**Accessing elements of a tensor**\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GbZuDYqg22n"},"outputs":[],"source":["## Accessing elements of a tensor\n","# Rank-3 tensor T has axes order (timestamps, patients, features)\n","\n","# Element of T at postion 3 w.r.t. dim-0, position 2 w.r.t. dim-1,\n","# position-1 w.r.t dim-2\n","print(T[3, 2, 1]) # BP of patient-2 at noon\n","\n","\n","# Element-0 of object T which is also the info for all patients at\n","# admission time 9AM\n","print(T[0]) # patients' info at admission time\n","print(T[-1]) # first element of T from the tail, patients' info at noon\n","\n","\n","# Patient-2 info at noon\n","T[-1, 2]\n"]},{"cell_type":"markdown","source":["---\n","\n","Understanding shapes\n","\n","---"],"metadata":{"id":"ekY1yrw62N7T"}},{"cell_type":"code","source":["#a = torch.tensor([1.0, 2.0, 3.0])\n","#a = torch.tensor([[1.0, 2.0, 3.0]])\n","a = torch.tensor([[[1.0, 2.0, 3.0]]])\n","print(a)\n","print(a.shape)"],"metadata":{"id":"SrvQxSe22SUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Broadcasting**\n","\n","---"],"metadata":{"id":"SW2_NDTCjIL5"}},{"cell_type":"code","source":["# A simple broadcasting example\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0])\n","print(a.shape)\n","print(b.shape)\n","print(a-b)"],"metadata":{"id":"1PjnkDnr_qSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How to add a new axis to a tensor using the unsqueeze() function\n","#print(T)\n","#print(T.shape)\n","T_patient0 = T[:, 0, :]\n","print(T_patient0)\n","print(T_patient0.shape)\n","print('---------')\n","T_patient0_new = torch.unsqueeze(T_patient0, 1)\n","print(T_patient0_new)\n","print(T_patient0_new.shape)\n","print('---------')\n","print(T)\n","print(T.shape)"],"metadata":{"id":"zhtxw34i_RNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How different are the patients from patient-0?\n","#T - T_patient0 # does not work for broadcasting\n","#T - T_patient0\n","\n","# How different are the patients compared to their time at admission\n","T-T_patient0_new"],"metadata":{"id":"DEPPWVsWjI4X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o6kEXfCpDzo"},"source":["---\n","\n","**Exercise**: interpret $\\texttt{T[:, -1, :]}$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6lEPZEWo6wo"},"outputs":[],"source":["# Last patient's info at all timestamps\n","T[:, -1, :]"]},{"cell_type":"markdown","source":["---\n","\n","Broadcasting exercise\n","\n","---"],"metadata":{"id":"9Zq396h0_5yB"}},{"cell_type":"code","source":["T = torch.randint(-5, 6, (4, 5, 3))\n","print(T)\n","v = torch.tensor([1.0, 2.0, 3.0])\n","print(v)"],"metadata":{"id":"-PqnqW9Z_-Sb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gc9EJuZQhD9i"},"source":["---\n","\n","$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert$ tells us how long a vector is. In 2-dimensions, $$\\mathbf{a}=\\begin{bmatrix}a_1\\\\a_2\\end{bmatrix}\\Rightarrow \\lVert\\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\mathbf{a}=\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_n\\end{bmatrix}\\Rightarrow\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n","\n","![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=500)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM65UP4_hEso"},"outputs":[],"source":["## l2 norm of a vector\n","x = torch.tensor([76.0, 124.0], dtype = torch.float64)\n","print(x)\n","torch.norm(x)"]},{"cell_type":"markdown","metadata":{"id":"SRbanrUmwLX7"},"source":["\n","---\n","\n","**Dot Product of Vectors**\n","\n","A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n","\n","The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s91XY1JZwU2w"},"outputs":[],"source":["## Dot product of vectors\n","a = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n","b = torch.tensor([4.0, 5.0, 6.0], dtype = torch.float64)\n","torch.dot(a, b)"]},{"cell_type":"markdown","metadata":{"id":"2-b90m-QXyFp"},"source":["---\n","\n","The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n","\n","![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=6000)\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GxZ95uXXz3P"},"outputs":[],"source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([2.0, 4.0])\n","c = torch.tensor([-2.0, 1.0])\n","d = torch.tensor([-1.0, -2.0])\n","print(torch.dot(a, b))\n","print(torch.dot(a, c))\n","print(torch.dot(a, d))"]},{"cell_type":"markdown","metadata":{"id":"U6CS4_8byCs8"},"source":["---\n","\n","Cauchy-Schwarz inequality $-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$\n","\n","This is a normalized measure of similarity (or extent of alignment) between vectors.\n","\n","Angle between vectors $\\mathbf{x}$ and $\\mathbf{y} = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right).$\n","\n","![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=400)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4UhBnPUx7TV"},"outputs":[],"source":["x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([2.0, 1.0])\n","\n","# Linear difference between x and y\n","print(torch.norm(x - y))\n","\n","# Angle difference between x and y in radians\n","print(torch.acos(torch.dot(x,y) / (torch.norm(x) * torch.norm(y))))\n","\n","# Angle difference between x and y in degrees\n","print((180.0/torch.pi)*(torch.acos(torch.dot(x,y) / (torch.norm(x) * torch.norm(y)))))"]},{"cell_type":"markdown","metadata":{"id":"1bnmEkg3Tctx"},"source":["---\n","\n","Application of the Cauchy-Schwarz inequality: is \"Cricket without Tendulkar\" same as \"Football without Messi\"?\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrmCknO5TkNZ"},"outputs":[],"source":["a = torch.tensor(model['cricket'] - model['tendulkar'], dtype = torch.float64)\n","b = torch.tensor(model['football'] - model['messi'], dtype = torch.float64)\n","\n","# Linear difference between and a and b\n","print(torch.norm(a-b))\n","\n","# Angle difference between a and b in radians\n","print(torch.acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b))))\n","\n","# Angle difference between a and b in degrees\n","print((180.0/torch.pi)*(torch.acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b)))))"]},{"cell_type":"code","source":["c = torch.tensor(model['soup'] - model['salt'], dtype = torch.float64)\n","\n","# Angle difference between a and b in degrees\n","print((180.0/torch.pi)*(torch.acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b)))))\n","\n","# Angle difference between a and c in degrees\n","print((180.0/torch.pi)*(torch.acos(torch.dot(a, c) / (torch.norm(a) * torch.norm(c)))))\n","\n","# cricket-without-tendulkar has approximately the same similarity w.r.t.\n","# football-without-messi and tennis-without-federer"],"metadata":{"id":"oDD91UnCHmbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayzM_0_synRF"},"source":["\n","---\n","\n","**Hadamard Product of Vectors**\n","\n","A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n","\n","The <font color=\"cyan\">$\\otimes$</font> represents the computation of the Hadamard product.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPojS0rIzR8p"},"outputs":[],"source":["## Hadamard product\n","a = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n","b = torch.tensor([4.0, 5.0, 6.0], dtype = torch.float64)\n","\n","# Element-wise multiplication (Hadamard product)\n","print(a*b)\n","print(torch.mul(a, b))"]},{"cell_type":"markdown","metadata":{"id":"oruyV_EjhqCR"},"source":["---\n","\n","A matrix-vector product is simply a sequence of dot products of the rows of the matrix (seen as vectors) with the vector\n","\n","![matvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=700)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_IScSWzhpi7"},"outputs":[],"source":["## Matrix-vector product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","x = torch.tensor([4.0, 2.0, -2.0])\n","\n","# Matrix-vector multiplication\n","print(torch.matmul(A, x))"]},{"cell_type":"markdown","metadata":{"id":"uTnGSJ3vT4EN"},"source":["---\n","\n","Here we create a simple sentence in English and tokenize it\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ73kkevT5L3"},"outputs":[],"source":["sentence = 'i swam quickly across the river to get to the other bank'\n","nltk.download('punkt_tab')\n"]},{"cell_type":"markdown","metadata":{"id":"M40pqI8UUbX4"},"source":["---\n","\n","Generate the word embeddings for the tokens and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mKKVRyxUh5V"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0Z0pZQisxtY-"},"source":["---\n","\n","A matrix-matrix product is simply a sequence of matrix-vector products.\n","\n","![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=550)\n","\n","\n","---"]},{"cell_type":"markdown","source":["---\n","\n","Matrix-matrix product using patient data matrix and a weights matrix:\n","\n","![patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n","\n","$$\\mathbf{Z} = \\mathbf{XW}.$$\n","\n","---"],"metadata":{"id":"h5cHHVQOuT0z"}},{"cell_type":"code","source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]], dtype = torch.float64)\n","print(f'Patient data matrix X:\\n {X}') #f-string in Python\n","\n","# Weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype = torch.float64)\n","print(f'Weights matrix:\\n {W}')\n","\n","# Raw scores matrix (matrix-matrix multiplication)\n","Z = torch.matmul(X, W)\n","print(f'Raw zcores matrix:\\n {Z}')\n","# The raw scores are also referred to as the logits"],"metadata":{"id":"njrrw_MnuUpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756543853641,"user_tz":-330,"elapsed":39,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}},"outputId":"18bf3054-b1b0-431f-a807-daa5d9aa6fec"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Patient data matrix X:\n"," tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n","       dtype=torch.float64)\n","Weights matrix:\n"," tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000]], dtype=torch.float64)\n","Raw zcores matrix:\n"," tensor([[ 16.2500, 113.5700, -44.6700],\n","        [ 47.2000, 114.3000, -27.0000],\n","        [  6.1500, 111.9000, -72.9500],\n","        [ 41.8000, 128.2000, -50.0000],\n","        [ 31.5500, 126.5200, -74.9700],\n","        [ 47.4000, 108.4800, -20.4800]], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**Version-1** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n","\n","*What a particular neuron understands about a particular patient.*\n","\n","![matrix-matrix product version-1](https://1drv.ms/i/c/37720f927b6ddc34/IQQdAOCwtndURKA-h4yvpTqlAYjBjlcweRSeMYkPvf7dwmQ?width=660)\n","\n","$$\\begin{align*}[\\mathbf{Z}]_{i,j} &= (i,j)\\text{-th element of }\\mathbf{Z}\\\\&=\\text{what the }j\\text{th neuron learns about the } i\\text{th patient}\\\\&=\\mathbf{x}^{(i)}\\cdot\\mathbf{w}_j\\\\& = {\\mathbf{x}^{(i)}}^\\mathrm{T}\\mathbf{w}_j\\\\\\Rightarrow \\underbrace{[\\mathbf{Z}]_{{\\color{yellow}0},{\\color{cyan}2}}}_{{\\color{yellow}0}\\text{th patient},\\,{\\color{cyan}2}\\text{nd neuron}} &= \\mathbf{x}^{({\\color{yellow}0})}\\cdot\\mathbf{w}_{{\\color{cyan}2}}\\\\ &= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}\\cdot\\begin{bmatrix}0.3\\\\0.5\\\\0.1\\\\-1.0\\\\-0.8\\end{bmatrix}\\\\ &= -44.67.\\end{align*}$$\n","\n","---"],"metadata":{"id":"qWigLvBRucwi"}},{"cell_type":"code","source":["## The (0, 2)-th element of the matrix-matrix product XW\n","torch.dot(X[0, :], W[:, 2])\n","#torch.matmul(X[0, :], W[:, 2]) (not recommended)"],"metadata":{"id":"q-rGT4NaueRk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Version-2** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n","\n","*What a particular neuron understands about all the patients.*\n","\n","![matrix-matrix product version-2](https://1drv.ms/i/c/37720f927b6ddc34/IQRm1-w-6TG0R4C4J4BizyzyAWIbcHzbEjgmx-0JFREdHsE?width=660)\n","\n","$$\\begin{align*}\\mathbf{z}_j &= \\mathbf{X}\\mathbf{w}_j\\\\&=\\text{what the } j\\text{th neuron learns about the all the patients}\\\\&=w_{j,0}\\times\\textbf{HR}+w_{j,1}\\times\\textbf{BP}+w_{j,2}\\times\\textbf{Temp}+w_{j,3}\\times\\textbf{Sugar}+w_{j,4}\\times\\textbf{Vitamin D}\\\\&= w_{j,0}\\mathbf{x}_0+w_{j,1}\\mathbf{x}_1+w_{j,2}\\mathbf{x}_2+w_{j,3}\\mathbf{x}_3+w_{j,4}\\mathbf{x}_4\\\\\\Rightarrow\\underbrace{\\mathbf{z}_{{\\color{cyan}0}}}_{{\\color{cyan}0}\\text{th neuron understanding}} &= \\underbrace{\\mathbf{X}}_{\\color{yellow}{\\text{all patients}}}\\ \\underbrace{\\mathbf{w}_{{\\color{cyan}0}}}_{{\\color{cyan}0}\\text{th neuron weights}}\\\\&= {\\color{cyan}{-0.1}}\\times\\begin{bmatrix}{\\color{yellow}{72}}\\\\{\\color{yellow}{85}}\\\\{\\color{yellow}{68}}\\\\{\\color{yellow}{90}}\\\\{\\color{yellow}{84}}\\\\{\\color{yellow}{78}}\\end{bmatrix}+{\\color{cyan}{0.9}}\\times\\begin{bmatrix}{\\color{yellow}{120}}\\\\{\\color{yellow}{130}}\\\\{\\color{yellow}{110}}\\\\{\\color{yellow}{140}}\\\\{\\color{yellow}{132}}\\\\{\\color{yellow}{128}}\\end{bmatrix}+({\\color{cyan}{-1.5}})\\times\\begin{bmatrix}{\\color{yellow}{37.3}}\\\\{\\color{yellow}{37.0}}\\\\{\\color{yellow}{38.5}}\\\\{\\color{yellow}{38.0}}\\\\{\\color{yellow}{38.3}}\\\\{\\color{yellow}{37.2}}\\end{bmatrix}+{\\color{cyan}{0.1}}\\times\\begin{bmatrix}{\\color{yellow}{104}}\\\\{\\color{yellow}{110}}\\\\{\\color{yellow}{125}}\\\\{\\color{yellow}{130}}\\\\{\\color{yellow}{146}}\\\\{\\color{yellow}{102}}\\end{bmatrix}+({\\color{cyan}{-1.2}})\\times\\begin{bmatrix}{\\color{yellow}{32.5}}\\\\{\\color{yellow}{14}}\\\\{\\color{yellow}{34}}\\\\{\\color{yellow}{26}}\\\\{\\color{yellow}{30}}\\\\{\\color{yellow}{12}}\\end{bmatrix}\\\\&=\\begin{bmatrix}16.25\\\\47.20\\\\6.15\\\\41.80\\\\31.55\\\\47.40\\end{bmatrix}.\\end{align*}$$\n","\n","\n","\n","---"],"metadata":{"id":"RzqALUS-ugoU"}},{"cell_type":"code","source":["## The 0-th column of the matrix-matrix product XW\n","torch.matmul(X, W[:, 0])"],"metadata":{"id":"sJbmVTzuukEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Version-3** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n","\n","*What all neurons understand about a particular patient.*\n","\n","![matrix-matrix product version-3](https://1drv.ms/i/c/37720f927b6ddc34/IQRfO-qEJQ9mQYLH_f-lyjeQAaWV4FrDjTjaEHJpPB1PmCg?width=660)\n","\n","$$\\begin{align*}{\\mathbf{z}^{(i)}}^\\mathrm{T}&={\\mathbf{x}^{(i)}}^\\mathrm{T}\\mathbf{W}\\\\&= \\text{what is learned about the }i\\text{th patient by all the neurons}\\\\&=i\\text{th HR }\\times{\\mathbf{w}^{(0)}}^\\mathrm{T}+i\\text{th BP }\\times{\\mathbf{w}^{(1)}}^\\mathrm{T}+i\\text{th Temp }\\times{\\mathbf{w}^{(2)}}^\\mathrm{T}+i\\text{th Sugar }\\times{\\mathbf{w}^{(3)}}^\\mathrm{T}+i\\text{th Vitamin D }\\times{\\mathbf{w}^{(4)}}^\\mathrm{T}\\\\&=x^{(i)}_0\\times{\\mathbf{w}^{(0)}}^\\mathrm{T}+x^{(i)}_1\\times{\\mathbf{w}^{(1)}}^\\mathrm{T}+x^{(i)}_2\\times{\\mathbf{w}^{(2)}}^\\mathrm{T}+x^{(i)}_3\\times{\\mathbf{w}^{(3)}}^\\mathrm{T}+x^{(i)}_4\\times{\\mathbf{w}^{(4)}}^\\mathrm{T}\\\\\\underbrace{\\Rightarrow{{\\mathbf{z}^{({\\color{yellow}0})}}^\\mathrm{T}}}_{{\\color{yellow}{0}}\\text{th patient understanding}}&=\\underbrace{{{\\mathbf{x}^{({\\color{yellow}0})}}^\\mathrm{T}}}_{{\\color{yellow}{0}}\\text{th patient}}\\ \\underbrace{\\mathbf{W}}_{{\\color{cyan}{\\text{all neurons}}}}\\\\ &= {\\color{yellow}{72}}\\times\\begin{bmatrix}{\\color{cyan}{-0.1}} & {\\color{cyan}{0.5}} & {\\color{cyan}{0.3}}\\end{bmatrix} \\\\&+ {\\color{yellow}{120}}\\times\\begin{bmatrix}{\\color{cyan}{0.9}} & {\\color{cyan}{0.3}} & {\\color{cyan}{0.5}}\\end{bmatrix}\\\\&+{\\color{yellow}{37.3}}\\times\\begin{bmatrix}{\\color{cyan}{-1.5}} & {\\color{cyan}{0.4}} & {\\color{cyan}{0.1}}\\end{bmatrix}\\\\&+{\\color{yellow}{104}}\\times\\begin{bmatrix}{\\color{cyan}{0.1}} & {\\color{cyan}{0.1}} & {\\color{cyan}{-1.0}}\\end{bmatrix}\\\\&+{\\color{yellow}{32.5}}\\times\\begin{bmatrix}{\\color{cyan}{-1.2}} & {\\color{cyan}{0.5}} & {\\color{cyan}{-0.8}}\\end{bmatrix}\\\\&=\\begin{bmatrix}16.25 & 113.57 & 7.33\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"],"metadata":{"id":"jrQE8b2xukgE"}},{"cell_type":"code","source":["Z"],"metadata":{"id":"Y0KMgb1KA7eE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## The 0-th row of the matrix-matrix product XW\n","torch.matmul(X[0, :], W)"],"metadata":{"id":"o-Nv7NOLun5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVoJRc6kUtI2"},"source":["---\n","\n","The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}_\\mathrm{word}$ is the matrix-matrix product $\\mathbf{X}_\\mathrm{word}\\mathbf{X}_\\mathrm{word}^\\mathrm{T}.$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ms9Qg5AoVJy_"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["---\n","\n","The softmax function: takes a $k$-vector $\\mathbf{z}$ as input and returns a vector $\\mathbf{a}$ of the same shape as the output which is referred to as the softmax-activated scores.\n","\n","$\\begin{align*}\\mathbf{a}&=\\text{softmax}(\\mathbf{z})=\\begin{bmatrix}\\dfrac{e^{z_1}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\\\\\dfrac{e^{z_2}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\\\\\vdots\\\\\\dfrac{e^{z_k}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\end{bmatrix}.\\end{align*}$\n","\n","In the following example, we consider a raw scores vector $\\mathbf{z}$ with 3 components which leads to the softmax-activated scores vectors $\\mathbf{a}$ which can be interpreted as the predicted probabilities that the sample belongs to each one of the output classes:\n","\n","![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n","\n","\n","---"],"metadata":{"id":"NLWq_5p3usNO"}},{"cell_type":"code","source":["z = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n","print(z)\n","softmax = torch.nn.Softmax(dim = 0)\n","a = softmax(z)\n","print(a)\n","print(torch.sum(a))"],"metadata":{"id":"hgbn1YaJc5i-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Raw scores matrix (matrix-matrix multiplication)\n","Z = torch.matmul(X, W)\n","print(f'Raw zcores matrix:\\n {Z}')\n","\n","# Calculate the softmax scores\n","softmax = torch.nn.Softmax(dim = 1)\n","A = softmax(Z)\n","print(f'Softmax scores matrix:\\n {A}')"],"metadata":{"id":"ILhIBVoKr3fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Standardization of data to get rid of the effects of units.\n","\n","The standard deviation of a vector is a measure of how much the components or elements of that vector typically deviate from their average value. For an $n$-vector $\\mathbf{x},$ the standard deviation is denoted and calculated as\n","$$\\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}\\Rightarrow\\text{std}(\\mathbf{x}) = \\sqrt{\\frac{\\left[x_1-\\text{avg}(\\mathbf{x})\\right]^2+\\cdots+\\left[x_n-\\text{avg}(\\mathbf{x})\\right]^2}{n}}.$$ The quantity inside the square root above is the average squared deviation which is also called the variance denoted as $$\\text{var}(\\mathbf{x}) = \\frac{\\left[x_1-\\text{avg}(\\mathbf{x})\\right]^2+\\cdots+\\left[x_n-\\text{avg}(\\mathbf{x})\\right]^2}{n}.$$\n","\n","This means $\\text{std}(\\mathbf{x}) = \\sqrt{\\text{var}(\\mathbf{x})}.$\n","\n","A large standard deviation indicates that the components of the vector typically deviate a lot from their average value or mean.\n","\n","The following component plot of a vector of heart rate values has the 1-standard deviation-above and below the mean represented as red-dotted lines:\n","\n","![standard deviation](https://1drv.ms/i/c/37720f927b6ddc34/IQQB_uF-TUO8SpoodLWz7sQPAc4POmYfY3hPjlX3vpYfKlY?width=540)\n","\n","---"],"metadata":{"id":"VCo2ano8mH_x"}},{"cell_type":"code","source":["# Heart rate vector\n","a = X[:, 0]\n","print(f'Heart rate vector:\\n {a}')\n","\n","# BP vector\n","b = X[:, 1]\n","#print(f'Blood pressure vector:\\n {b}')\n","\n","# Average heart rate\n","print(f'Average heart rate: {torch.mean(a)}')\n","\n","# Average BP\n","#print(torch.mean(b))\n","\n","# Mean-centered heart rate vector or the de-meaned heart rate vector or the\n","# deviations in heart rate vectors\n","a_mc = a - torch.mean(a)\n","print(f'Deviations in heart rate vector:\\n {a_mc}')\n","\n","# The average of the components of the mean-centered heart rate vector is zero\n","#print(torch.mean(a_mc))\n","\n","# The squared deviations vector\n","print(f'Squared-deviations in heart rate vector:\\n {a_mc**2}')\n","\n","# The average of the squared deviations vector a.k.a. the variance in\n","# the heart rate\n","v = torch.mean(a_mc**2)\n","print(f'Average squared deviation or variance in the heart rate: {v}')\n","\n","# Square-root of the average of the squared deviations vector\n","# which is the same as the square root of the variance a.k.a. the\n","# standard deviation in the heart rate\n","s = torch.sqrt(v)\n","print(f'Standard deviation of the heart rate: {s}')\n","\n","# Standardized heart rate vector a.k.a. the z-scores of the heart rate is\n","# obtained by subtracting the mean heart rate and dividing by the\n","# deviation of the heart rates\n","z = a_mc / s #same as (a-np.mean(a)) / np.std(a)\n","print(f'Standardized heart rate vector:\\n{z}')"],"metadata":{"id":"IioKqqpymL2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Suppose heart rate is measured in beats per hour instead of beats per minute. How do the z-scores look like now?\n","\n","---"],"metadata":{"id":"qMaz_7oNMFYM"}},{"cell_type":"code","source":["# Suppose heart rate is measured in beats per hour instead of beats per minute\n","a = X[:, 0]*60\n","print(f'Heart rate vector:\\n {a}')\n","\n","# BP vector\n","b = X[:, 1]\n","#print(f'Blood pressure vector:\\n {b}')\n","\n","# Average heart rate\n","print(f'Average heart rate: {torch.mean(a)}')\n","\n","# Average BP\n","#print(torch.mean(b))\n","\n","# Mean-centered heart rate vector or the de-meaned heart rate vector or the\n","# deviations in heart rate vectors\n","a_mc = a - torch.mean(a)\n","print(f'Deviations in heart rate vector:\\n {a_mc}')\n","\n","# The average of the components of the mean-centered heart rate vector is zero\n","#print(torch.mean(a_mc))\n","\n","# The squared deviations vector\n","print(f'Squared-deviations in heart rate vector:\\n {a_mc**2}')\n","\n","# The average of the squared deviations vector a.k.a. the variance in\n","# the heart rate\n","v = torch.mean(a_mc**2)\n","print(f'Average squared deviation or variance in the heart rate: {v}')\n","\n","# Square-root of the average of the squared deviations vector\n","# which is the same as the square root of the variance a.k.a. the\n","# standard deviation in the heart rate\n","s = torch.sqrt(v)\n","print(f'Standard deviation of the heart rate: {s}')\n","\n","# Standardized heart rate vector a.k.a. the z-scores of the heart rate\n","z = a_mc / s\n","print(f'Standardized heart rate vector:\\n{z}')\n","# The z-scores are the same as before when the heart rate was in beats per minute"],"metadata":{"id":"NuCt2XMjzwj9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","One-hot encoding of the true output labels\n","\n","\n","![patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n","\n","---"],"metadata":{"id":"qA6qAaCW9RQ_"}},{"cell_type":"code","source":["# The following does not work in PyTorch\n","#y = torch.tensor(['non-diabetic', 'diabetic'])\n","\n","# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n","# PyTorch which itself is equivalent to a vector in pen & paper)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","print(y)\n","print(type(y))\n","print(y.shape)\n","y = y.reshape(-1, 1)\n","print('------')\n","print(y)\n","print(type(y))\n","print(y.shape)\n","print('-------')\n","# Creating a one-hot encoder object\n","ohe = OneHotEncoder(sparse_output = False)\n","# Create the one-hot encoded true output labels matrix\n","Y = torch.tensor(ohe.fit_transform(y), dtype = torch.float64)\n","print(Y)"],"metadata":{"id":"7ABCvj-19baz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756543871257,"user_tz":-330,"elapsed":30,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}},"outputId":"1bc3872f-3e7e-483f-8db0-4e8ea136ab8c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['non-diabetic' 'diabetic' 'non-diabetic' 'pre-diabetic' 'diabetic'\n"," 'pre-diabetic']\n","<class 'numpy.ndarray'>\n","(6,)\n","------\n","[['non-diabetic']\n"," ['diabetic']\n"," ['non-diabetic']\n"," ['pre-diabetic']\n"," ['diabetic']\n"," ['pre-diabetic']]\n","<class 'numpy.ndarray'>\n","(6, 1)\n","-------\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","The forward propagation\n","\n","---"],"metadata":{"id":"tBg3HSbDFUQT"}},{"cell_type":"code","source":["# Standardize the data\n","sc = StandardScaler() # create a standard scaler object\n","X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n","print(f'The standardized data matrix:\\n{X_std}')\n","\n","# The one-hot encoded true output labels matrix\n","print(f'One-hot encoded true output labels matrx:\\n{Y}')\n","\n","# Calculate the raw scores using the standardized data matrix\n","# and the weights matrix\n","print(f'The weights matrix:\\n{W}')\n","Z = torch.matmul(X_std, W)\n","print(f'The raw scores matrix:\\n{Z}')\n","\n","# Calculate the softmax-activated scores matrix\n","softmax = torch.nn.Softmax(dim = 1)\n","A = softmax(Z)\n","print(f'The softmax-activated raw scores matrix:\\n{A}')\n","\n","# Quantify the unhappiness w.r.t. the current set of weights\n","print(f'One-hot encoded true output labels matrix:{Y}')\n","print(f'Hadamard product of Y and A:{Y*A}')\n","print(torch.sum(Y*A, dim=1))\n","print(-torch.log(torch.sum(Y*A, dim=1)))\n","print(torch.mean(-torch.log(torch.sum(Y*A, dim=1))))\n","# Calculate the average training loss\n","L = torch.mean(-torch.log(torch.sum(Y*A, dim=1)))\n","print(f'Average training loss = {L}')"],"metadata":{"id":"aBIbqmivFXa0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A detour to data structures in Python\n","\n","---"],"metadata":{"id":"xHosp-fHBRqA"}},{"cell_type":"code","source":["# PyTorch tensor\n","a = torch.tensor([1.0, 2.0, 3.0])\n","print(a)\n","print(type(a))\n","print(a.shape)\n","print('-------')\n","\n","# Numpy array\n","b = np.array([1.0, 2.0, 3.0])\n","print(b)\n","print(type(b))\n","print(b.shape)\n","print('------')\n","\n","# List\n","c = [1.0, 2.0, 3.0]\n","print(c)\n","print(type(c))\n","#print(c.shape) # does not work\n","print('------')\n","\n","# Tuple\n","d = (1.0, 2.0, 3.0)\n","print(d)\n","print(type(d))\n","#print(d.shape) # does not work"],"metadata":{"id":"_RUy0ovx_DD6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Read hourly temperature data for multiple cities\n","\n","---"],"metadata":{"id":"g6oLUxSbibvy"}},{"cell_type":"code","source":["## Read hourly temperature data for multiple cities\n","FILE = DATA_DIR + 'temperature.csv'\n","df_temp = pd.read_csv(FILE, sep = \",\", header = 0, skiprows = [1])\n","df_temp['datetime'] = pd.to_datetime(df_temp['datetime'], format='%Y-%m-%d %H:%M:%S')\n","df_temp = df_temp.set_index('datetime')\n","df_temp.head()"],"metadata":{"id":"WxY-tjdyigHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Extract daily temperature vectors for San Francisco for October 2, 2012 and December 2, 2012 and make component plots of those vectors\n","\n","---"],"metadata":{"id":"OOfaJiJOjVfz"}},{"cell_type":"code","source":["df_temp['Vancouver']\n","df_temp['Vancouver'].values\n","df_temp.index.get_loc('2012-10-02')\n","df_temp.columns.get_loc('San Francisco')\n","df_temp.iloc[(df_temp.index.get_loc('2012-10-02')),df_temp.columns.get_loc('San Francisco')]"],"metadata":{"id":"98iP0oJaB2mF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Temperature vector for San Francisco for 2012-10-02\n","t1 = df_temp.iloc[df_temp.index.get_loc('2012-10-02'), df_temp.columns.get_loc('San Francisco')].values\n","t1 = t1-273.15\n","print(t1)\n","\n","# Temperature vector for San Francisco for 2012-12-02\n","t2 = df_temp.iloc[(df_temp.index.get_loc('2012-12-02')),df_temp.columns.get_loc('San Francisco')].values\n","t2 = t2-273.15\n","print(t2)\n","\n","# Plot temperature vectors for both days\n","fig,(ax1,ax2)=plt.subplots(2,1,figsize=(6,8))\n","component_index=range(0,len(t1))\n","\n","ax1.plot(component_index,t1,color = 'green',marker='*')\n","ax1.plot(component_index,[np.mean(t1)]*len(t1),color='blue',linestyle='dashed',linewidth=1)\n","ax1.plot(component_index,[np.mean(t1)-np.std(t1)]*len(t1),color='red',linestyle='dashed',linewidth=1)\n","ax1.plot(component_index,[np.mean(t1)+np.std(t1)]*len(t1),color='red',linestyle='dashed',linewidth=1)\n","ax1.set_xlabel('Hour')\n","ax1.set_ylabel('Temperature (celsius)')\n","ax1.set_title('Temperature in Sac Fransisco for 2012-10-02')\n","\n","ax2.plot(component_index,t2,color = 'green',marker='*')\n","ax2.plot(component_index,[np.mean(t2)]*len(t2),color='blue',linestyle='dashed',linewidth=1)\n","ax2.plot(component_index,[np.mean(t2)-np.std(t2)]*len(t2),color='red',linestyle='dashed',linewidth=1)\n","ax2.plot(component_index,[np.mean(t2)+np.std(t2)]*len(t2),color='red',linestyle='dashed',linewidth=1)\n","ax2.set_ylabel('Hour')\n","ax2.set_ylabel('Temperature (celsius)')\n","ax2.set_title('Temperature in Sac Fransisco for 2012-12-02')\n","ax2.plot(component_index,t2)\n"],"metadata":{"id":"pCX3O2DkjXVU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Mean-center the daily temperature vectors for San Francisco for October 2, 2012 and December 2, 2012 and make component plots of those vectors\n","\n","---"],"metadata":{"id":"0XHj06rFot3v"}},{"cell_type":"code","source":["# Mean-centered temperature vector for San Francisco for 2012-10-02\n","t1_mc = t1 - np.mean(t1)\n","print(t1_mc)\n","\n","# Mean-centered temperature vector for San Francisco for 2012-12-02\n","t2_mc = t2 - np.mean(t2)\n","print(t2_mc)\n","\n","# Plot mean-centered temperature vectors for both days\n","fig, (gx1, gx2) = plt.subplots(2, 1, figsize = (6, 10))\n","component_index = range(0, len(t1_mc))\n","\n","gx1.plot(component_index, t1_mc, color = 'green', marker = '*')\n","gx1.plot(component_index, [np.mean(t1_mc)] * len(t1_mc),color = 'blue',\n","         linestyle = 'dashed', linewidth = 1)\n","gx1.plot(component_index, [np.mean(t1_mc)-np.std(t1_mc)] * len(t1_mc), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","gx1.plot(component_index, [np.mean(t1_mc)+np.std(t1_mc)] * len(t1_mc), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","\n","\n","gx1.set_xlabel('Hour')\n","gx1.set_ylabel('Temperature(celsius)')\n","gx1.set_title('Temperature in San Francisco on october 2, 2012')\n","\n","gx2.plot(component_index, t2_mc, color = 'green', marker = '*')\n","gx2.plot(component_index, [np.mean(t2_mc)] * len(t2_mc), color = 'blue',\n","         linestyle = 'dashed', linewidth = 1)\n","\n","gx2.plot(component_index, [np.mean(t2_mc)-np.std(t2_mc)] * len(t2), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","gx2.plot(component_index, [np.mean(t2_mc)+np.std(t2_mc)] * len(t2), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","gx2.set_xlabel('Hour')\n","gx2.set_ylabel('Temperature(celsius)')\n","gx2.set_title('Temperature in San Francisco on december 2, 2012')"],"metadata":{"id":"J7pJlYVqpDBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Standardize daily temperature vectors for San Francisco for October 2, 2012 and December 2, 2012 and make component plots of those vectors\n","\n","---"],"metadata":{"id":"xA2Vzv-kphSq"}},{"cell_type":"code","source":["# Standardized temperature vector for San Francisco for 2012-10-02\n","z1 = t1_mc / np.std(t1_mc)\n","print(z1)\n","\n","# Mean-centered temperature vector for San Francisco for 2012-12-02\n","z2 = t2_mc / np.std(t2_mc)\n","print(z2)\n","\n","# Plot standardized temperature vectors for both days\n","fig, (kx1, kx2) = plt.subplots(2, 1, figsize = (6, 10))\n","component_index = range(0, len(z1))\n","\n","kx1.plot(component_index, z1, color = 'green', marker = '*')\n","kx1.plot(component_index, [np.mean(z1)] * len(z1), color = 'blue',\n","         linestyle = 'dashed', linewidth = 1)\n","\n","kx1.plot(component_index, [np.mean(z1)-np.std(z1)] * len(t1), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","kx1.plot(component_index, [np.mean(z1)+np.std(z1)] * len(t1), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","kx1.set_xlabel('Hour')\n","kx1.set_ylabel('Temperature(celsius)')\n","kx1.set_title('Temperature in San Francisco on october 2, 2012')\n","\n","kx2.plot(component_index, z2, color = 'green', marker = '*')\n","kx2.plot(component_index, [np.mean(z2)] * len(z2), color = 'blue',\n","         linestyle = 'dashed', linewidth = 1)\n","\n","kx2.plot(component_index, [np.mean(z2)-np.std(z2)] * len(z2), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","kx2.plot(component_index, [np.mean(z2)+np.std(z2)] * len(z2), color = 'red',\n","         linestyle = 'dashed', linewidth = 1)\n","kx2.set_xlabel('Hour')\n","kx2.set_ylabel('Temperature(celsius)')\n","kx2.set_title('Temperature in San Francisco on december 2, 2012')"],"metadata":{"id":"zasEwqTKptuz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Identify outliers in the daily temperature vectors for San Francisco for October 2, 2012 and December 2, 2012\n","\n","---"],"metadata":{"id":"p4JI3Vxvp95B"}},{"cell_type":"code","source":["threshold = 2.0\n","print(z1)\n","np.abs(z1) >= threshold\n","np.where(np.abs(z1) >= threshold)\n","np.where(np.abs(z2) >= threshold)"],"metadata":{"id":"3okoL5-XbagZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Z-score threshold\n","threshold = 2.0 # number of standard deviation beyond which samples are outliers\n","print(f'On 2012-10-02, outliers hours were: {np.where(np.abs(z1) >= threshold)}')\n","print(f'On 2012-12-02, outliers hours were: {np.where(np.abs(z2) >= threshold)}')"],"metadata":{"id":"pngfJpRLqHpz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Download popular stock prices data from Yahoo Finance for the range August 1, 2024 to August 1, 2025\n","\n","---"],"metadata":{"id":"XRhsABfMv6bX"}},{"cell_type":"code","source":["# Download stock price data of popular companies\n","stocks = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'JNJ', 'JPM', 'NVDA', 'AMZN', 'META', 'XOM']\n","dfstock = yf.download(stocks, start = \"2024-08-01\", end = \"2025-08-01\")\n","dfstock = dfstock.xs('Close', axis = 1, level = 0)\n","dfstock.dropna(inplace = True)\n","dfstock.head()"],"metadata":{"id":"uFLvp6n6wNnz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Compute daily log returns as $\\log\\left(\\frac{P_t}{P_{t-1}}\\right)$ where $P_t$ is stock price today and $P_{t-1}$ is stock price yesterday. For each company, its log return values can be treated as a vector."],"metadata":{"id":"b6fWdwuwzcBo"}},{"cell_type":"code","source":["# Compute log returns\n","log_returns = np.log(dfstock / dfstock.shift(1))\n","log_returns.dropna(inplace = True)\n","log_returns.head()\n"],"metadata":{"id":"lxkvKEC7zx7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The risk or volatility associated with a stock can be quantified by the standard deviation of the log returns of that stock.\n","\n","First, compute the volatility in the log returns of TSLA. Then, compute the volatility in the log returns of the all the stocks and plot them in a bar plot in descending order.\n","\n","---"],"metadata":{"id":"lwZNy-Nxz9aP"}},{"cell_type":"code","source":["# Compute standard deviation a.k.a. volatility a.k.a. risk of TSLA\n","a = log_returns['TSLA'].values\n","volatility_TSLA = np.sqrt(np.dot(a-np.mean(a), a-np.mean(a)/len(a)))# np.std(a)\n","print(f'Volatility in the log returns of TSLA = {volatility_TSLA}')\n","\n","# All stocks volatilities\n","volatilities = log_returns.std()\n","volatilities = volatilities.sort_values(ascending = False)\n","print(f'Volatility in the log returns of all stocks =\\n {volatilities}')\n","\n","# Plot volatilities\n","fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n","ax.bar(volatilities.index, volatilities.values)\n","ax.set_xlabel('Company')\n","ax.set_ylabel('Volatility')\n","ax.set_title('Volatility (Std Dev) of Stocks');"],"metadata":{"id":"Z-XM7M-_0Bm1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The covariance between two vectors is a measure of how the two vectors co-vary about their respective means. It is calculated as the average dot product of their mean-centered versions. If $\\mathbf{a}_{mc}$ and $\\mathbf{b}_{mc}$ represent the mean-centered versions of vectors $\\mathbf{a}$ and $\\mathbf{b},$ respectively, then the covariance between $\\mathbf{a}$ and $\\mathbf{b}$ is denoted and calculated as $$\\text{cov}(\\mathbf{a},\\mathbf{b}) = \\frac{1}{n}\\times\\left(\\mathbf{a}_{mc}\\cdot\\mathbf{b}_{mc}\\right).$$\n","\n","In the component plot below of mean-centered heart rate and blood pressure values, it can be seen that the heart rates and blood pressures of the patients are co-varying similarly; that is, as the heart rate increases or decreases w.r.t. the average heart rate from one patient to another, the blood pressure also typically does the same w.r.t the average blood pressure.\n","\n","![Covariance](https://1drv.ms/i/c/37720f927b6ddc34/IQTLFeKh-TzhSpSSm3swzRsaAarPH-HPV-j9tmHPV7eiDuY?width=375&height=580)\n","\n","Calculate the covariance between the log returns of AAPL and AMZN. Does it indicate that those stocks co-vary similarly?\n","\n","Make component plots of the log returns of AAPL and AMZN. Do you see the stocks co-varying similarly?\n","\n","Modify the component plots to show the mean-centered log returns. Now do you see the stocks co-varying similarly?\n","\n","\n","\n","---"],"metadata":{"id":"siWXCKtjBiAV"}},{"cell_type":"code","source":["# Covariance between log returns of APPL and AMZN\n","a = log_returns['AAPL'].values\n","a_mc = a-np.mean(a)\n","b = log_returns['AMZN'].values\n","b_mc = b-np.mean(b)\n","cov = np.cov(a,b)\n","print(f'Covariance between AAPL and AMZN log returns = {cov}');\n","\n","# Component plot of the log returns of AAPL and AMZN\n","fig, ax = plt.subplots(1, 1, figsize = (6, 10))\n","component_index = range(0, len(log_returns))\n","ax.plot(component_index, a, color = 'red', marker = '*')\n","ax.plot(component_index, b, color = 'blue', marker = '*')\n","ax.set_xlabel('Day')\n","ax.set_ylabel('Log Returns')\n","ax.set_title( 'Log Returns of AAPL and AMZN');"],"metadata":{"id":"Z0TCVUv9B2RS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Compute the covariance between all pairs of log returns. The resulting matrix, denoted as $\\pmb{\\Sigma}$ is called the covariance matrix.\n","\n","From that matrix, are you able to identify stocks that co-vary?\n","\n","---"],"metadata":{"id":"WJ2_XcGAY2Lc"}},{"cell_type":"code","source":["# Covariance matrix matrix of log returns\n","covmatrix = log_returns.cov()\n","print(covmatrix)"],"metadata":{"id":"w8oGMdzlZB0G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The correlation coefficient between two vectors $\\mathbf{a}$ and $\\mathbf{c}$ is a normalized measure (between -1 and 1) of their covariance which is denoted and calculated as $$\\text{cor}(\\mathbf{a},\\mathbf{b}) = \\frac{\\left(\\mathbf{a}_{mc}\\cdot\\mathbf{b}_{mc}\\right)}{\\lVert \\mathbf{a}_{mc}\\rVert_2\\lVert\\mathbf{b}_{mc}\\rVert_2}.$$ Note that the cauchy-Schwarz inequality guarantees that the above quantity is between -1 and 1.\n","\n","If the correlation coefficient is close to 1, then as one vector's components increase, the other vectors components also increase typically along a straight line. If the correlation coefficient is close to -1, then as one vector's components increase, the other vectors components decrease typically along a straight line.\n","\n","Compute the correlation coefficient between the log returns of AAPL and AMZN. Does it indicate that those stocks co-vary similarly?\n","\n","Make a scatter plot of the log returns of AAPL and AMZN. Does the plot indicate that the two stocks co-vary similarly?\n","\n","---"],"metadata":{"id":"4TBk1_U24Cy1"}},{"cell_type":"code","source":["# Correlation between APPL and AMZN)\n","corr = np.corrcoef(a,b)\n","print(f'Correation coefficient between AAPL and AMZN log returns = {corr}')\n","\n","# Scatter plot of the log returns of AAPL and AMZN\n","fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n","ax.scatter(log_returns['AMZN'], log_returns['AAPL'], s=10)\n","ax.set_xlabel('AMZN')\n","ax.set_ylabel('AAPL')\n","ax.set_title('Log Returns of Stock Prices');"],"metadata":{"id":"LAY02c5Q7vgQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Compute the correlation coefficient between all pairs of log returns and visualize the resulting correlation matrix. From that, identify stocks that co-vary typically.\n","\n","---"],"metadata":{"id":"l1Mr7Em1Urnf"}},{"cell_type":"code","source":["# Correlation matrix of log returns\n","corrmatrix = log_returns.corr()\n","#print(corrmatrix)\n","sns.heatmap(corrmatrix ,cmap='coolwarm',annot= True);\n","plt.title(\"Correlation Between Stocks\")\n","plt.show()"],"metadata":{"id":"7WInIgqNUsH7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The average log return (or simply return) associated with a stock (AAPL) is simply the average or mean of the log returns of the that stock. For example, if we denote the log returns of AAPL as an $n$-vector $\\mathbf{a},$ then its average log return in pen & paper can be written as $$\\frac{1}{n}\\times\\left(a_1+a_2+\\cdots+a_n\\right) = \\frac{1}{n}\\times\\left(\\pmb{1}\\cdot\\mathbf{a}\\right),$$ where $\\pmb{1}$ is an $n$-vector whose elements are all equal to ones.\n","\n","For a blended investment on the 10 stocks (called a portfolio) with weights that add up to 1 and represented as the vector $$\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\\\\\vdots\\\\w_{10}\\end{bmatrix},$$\n","- the averate return can be shown to be $\\mathbf{w}\\cdot\\mathbf{r},$ where $\\mathbf{r}$ is the vector of average returns of all the 10 stocks;\n","- the overall risk (or simply risk) in the portfolio is $\\mathbf{w}\\cdot\\left(\\pmb{\\Sigma}\\mathbf{w}\\right),$ which takes into account the individual volatility of each stock and their co-movement w.r.t. the other stocks.\n","\n","Try to come up with a portfolio (that is, values for the components of the weights vector $\\mathbf{w})$ that results in the best return-to-risk ratio. The best return-to-risk ratio is also referred to as the sharpe ratio.\n","\n","---"],"metadata":{"id":"PEsZixQiWfeJ"}},{"cell_type":"code","source":["# Optimizing portfolio\n","print(np.mean(log_returns['AAPL']))\n","print(np.std(log_returns['AAPL']))\n"],"metadata":{"id":"oX-tMCqIc7ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w=[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]\n"],"metadata":{"id":"wy4EZLnL3L4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def optimize_weights(r, Sigma, initial_w, epochs=5000, lr=0.01):\n","    \"\"\"\n","    Optimize portfolio weights using PyTorch to maximize Sharpe ratio\n","    based on loss = -(w⋅r)/(w⋅Σw).\n","\n","    Args:\n","        r: numpy array of mean returns (shape: [n])\n","        Sigma: numpy covariance matrix (shape: [n, n])\n","        initial_w: numpy array of initial weights (shape: [n])\n","        epochs: number of training iterations\n","        lr: learning rate for optimizer\n","\n","    Returns:\n","        optimal weights, average return, risk, Sharpe ratio\n","    \"\"\"\n","\n","    # Convert to torch tensors\n","    r_torch = torch.tensor(r, dtype=torch.float32)\n","    Sigma_torch = torch.tensor(Sigma, dtype=torch.float32)\n","\n","    n = len(r)\n","    # Initialize raw weights based on initial_w using log transform (to work with softmax)\n","    raw_w = torch.tensor(initial_w, dtype=torch.float32)\n","    raw_w = torch.log(raw_w + 1e-8) # log transform to allow softmax inverse\n","    raw_w.requires_grad_(True)\n","\n","    # Optimizer\n","    optimizer = torch.optim.Adam([raw_w], lr=lr)\n","\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","\n","        # Apply softmax so weights sum to 1 and are positive\n","        w = torch.softmax(raw_w, dim=0)\n","\n","        # Compute portfolio return and risk\n","        port_return = torch.dot(w, r_torch)\n","        port_risk = torch.dot(w, torch.mv(Sigma_torch, w)) # wᵀ Σ w\n","\n","        # Loss = negative Sharpe ratio\n","        loss = -(port_return / port_risk)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        if epoch % 500 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss.item()}, Sharpe Approx: {-loss.item()}\")\n","\n","    # Final weights\n","    final_w = torch.softmax(raw_w, dim=0).detach().numpy()\n","\n","    # Compute final metrics\n","    avg_return = (final_w @ r)\n","    risk = (final_w @ Sigma @ final_w)\n","    sharpe_ratio = avg_return / risk # As per your formula\n","\n","    return final_w, avg_return, risk, sharpe_ratio\n","\n","\n","# Example usage with your data\n","r = log_returns.mean().values\n","Sigma = log_returns.cov().values\n","\n","# Your initial weights\n","initial_w = np.array([0,0,0,0.5,0.2,0,0.15,0.15,0,0])\n","\n","optimal_w, avg_return, risk, sharpe_ratio = optimize_weights(r, Sigma, initial_w)\n","\n","print(\"\\nOptimal weights:\", optimal_w)\n","print(f\"Average return: {avg_return}\")\n","print(f\"Risk: {risk}\")\n","print(f\"Sharpe ratio: {sharpe_ratio}\")"],"metadata":{"id":"_HIqzn3Uzzxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Expected portfolio returns vector\n","r= torch.tensor(log_returns.mean().values, dtype=torch.float64)\n","\n","# initial portfolio weights vector\n","w= torch.tensor(0.1*np.ones(10),dtype= torch.float64 , requires_grad= True)\n","\n","# Covariance matrix of risks\n","s= torch.tensor(np.array(log_returns.cov()),dtype= torch.float64)\n","\n","# Define optimiser\n","optimizer= torch.optim.Adam([w],lr=1e-02)\n","\n","# loss function\n","def loss_fn(w):\n","    loss = -torch.dot(w,r)/torch.dot(w,torch.matmul(s,w))\n","    return loss\n","\n","# Optimization loop\n","num_epochs=10000\n","for epoch in range(num_epochs):\n","    # zero out the gradients\n","    optimizer.zero_grad()\n","\n","    # loss calculation\n","    w_constrained = torch.softmax(w,dim=0) # constraint satisfaction\n","    loss= loss_fn(w_constrained)\n","\n","    #backward propagation and optimization\n","    loss.backward()\n","    optimizer.step()\n","\n","  # Print the loss every 1000 epochs\n","    if epoch%1000 == 0:\n","        print(f'Epoch {epoch}, Loss: {loss.item()}')\n","\n","# Print the optimized portfolio weights vector\n","w=torch.softmax(w,dim=0)\n","print(w)\n","\n"],"metadata":{"id":"3UaesOtv4pDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimal sharpe ratio\n","average_return = torch.dot(w,r)\n","overall_risk = torch.dot(w,torch.matmul(s,w))\n","print(f'optimal sharpe_ratio = {average_return/overall_risk}')"],"metadata":{"id":"6TLy-hQ56_g7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the data\n","sc = StandardScaler() # create a standard scaler object\n","X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n","# print(f'The standardized data matrix:\\n{X_std}')\n","\n","# The one-hot encoded true output labels matrix\n","# print(f'One-hot encoded true output labels matrx:\\n{Y}')\n","\n","# Calculate the raw scores using the standardized data matrix\n","# and the weights matrix\n","# print(f'The weights matrix:\\n{W}')\n","# Z = torch.matmul(X_std, W)\n","# print(f'The raw scores matrix:\\n{Z}')\n","\n","# Calculate the softmax-activated scores matrix\n","# softmax = torch.nn.Softmax(dim = 1)\n","# A = softmax(Z)\n","# print(f'The softmax-activated raw scores matrix:\\n{A}')\n","\n","# Quantify the unhappiness w.r.t. the current set of weights\n","# print(f'One-hot encoded true output labels matrix:{Y}')\n","# print(f'Hadamard product of Y and A:{Y*A}')\n","# print(torch.sum(Y*A, dim=1))\n","# print(-torch.log(torch.sum(Y*A, dim=1)))\n","# print(torch.mean(-torch.log(torch.sum(Y*A, dim=1))))\n","# Calculate the average training loss\n","# L = torch.mean(-torch.log(torch.sum(Y*A, dim=1)))\n","# print(f'Average training loss = {L}')\n","\n","num_epochs=10000\n","for epoch in range(num_epochs):\n","    # zero out the gradients\n","    optimizer.zero_grad()\n","\n","    # loss calculation\n","    sc_constrained = torch.softmax(sc,dim=0) # constraint satisfaction\n","    loss= loss_fn(sc_constrained)\n","\n","    #backward propagation and optimization\n","    loss.backward()\n","    optimizer.step()\n","\n","  # Print the loss every 1000 epochs\n","    if epoch%1000 == 0:\n","        print(f'Epoch {epoch}, Loss: {loss.item()}')\n","\n","# Print the optimized portfolio weights vector\n","w=torch.softmax(sc,dim=0)\n","print(w)"],"metadata":{"id":"ybojjp3NYDfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]], dtype = torch.float64)\n","print(f'Patient data matrix X:\\n {X}') #f-string in Python\n","\n","# Weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype = torch.float64, requires_grad= True)\n","print(f'Weights matrix:\\n {W}')\n","\n","# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n","# PyTorch which itself is equivalent to a vector in pen & paper)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","y = y.reshape(-1, 1)\n","#Y = torch.tensor(ohe.fit_transform(y), dtype = torch.float64)\n","\n","\n","# Creating a one-hot encoder object\n","ohe = OneHotEncoder(sparse_output = False)\n","# Create the one-hot encoded true output labels matrix\n","Y = torch.tensor(ohe.fit_transform(y), dtype = torch.float64)\n","print(f'One-hot encoded true output labels matrx:\\n{Y}')\n","\n","\n","# Standardize the data\n","sc = StandardScaler() # create a standard scaler object\n","X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n","print(f'The standardized data matrix:\\n{X_std}')\n","\n","# Define optimizer\n","optimizer = torch.optim.Adam([W], lr = 1e-02)\n","\n","# loss function\n","def loss_fn(W):\n","  # loss = -torch.dot(w,r) / torch.dot(w, torch.matmul(S, w))\n","  # return loss\n","  Z = torch.matmul(X_std, W)\n","\n","\n","# Softmax-activated scores\n","  softmax = torch.nn.Softmax(dim = 1)\n","  A = softmax(Z)\n","\n","# Calculate the average trainig loss\n","  L = torch.mean(-torch.log(torch.sum(Y*A, dim=1)))\n","  return L\n","\n","\n","# Optimization loop\n","num_epochs = 10000 # epochs is number of iterations\n","for epoch in range(num_epochs):\n","# zero out the gradients\n","  optimizer.zero_grad()\n","\n","  # loss calculations\n","  #w_constrained = torch.softmax(W, dim=0) # constraints satisfaction happens\n","  loss = loss_fn(W)\n","\n","  # backpropagatio and optimization\n","  loss.backward()\n","  optimizer.step()\n","\n","  # print the loss of every 1000 epochs\n","  if epoch%1000 == 0:\n","    print(f'Epoch {epoch}, loss= {loss.item()}')\n","\n","# print the optmized portfolio weights vector\n","print(W)\n","\n"],"metadata":{"id":"tooZBbXUgG_x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756543894545,"user_tz":-330,"elapsed":13130,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}},"outputId":"df58eece-46bf-4075-eed4-fe2bbed289bd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Patient data matrix X:\n"," tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n","       dtype=torch.float64)\n","Weights matrix:\n"," tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000]], dtype=torch.float64, requires_grad=True)\n","One-hot encoded true output labels matrx:\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]], dtype=torch.float64)\n","The standardized data matrix:\n","tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676]], dtype=torch.float64)\n","Epoch 0, loss= 1.2303940464309857\n","Epoch 1000, loss= 0.018399732280405298\n","Epoch 2000, loss= 0.005553742501022239\n","Epoch 3000, loss= 0.0024706458206458126\n","Epoch 4000, loss= 0.0012747228155419502\n","Epoch 5000, loss= 0.0007065782346071862\n","Epoch 6000, loss= 0.00040700132319583683\n","Epoch 7000, loss= 0.0002395832067849751\n","Epoch 8000, loss= 0.00014282078783290256\n","Epoch 9000, loss= 8.57742685444377e-05\n","tensor([[  6.7107,  -2.7438,   1.0328],\n","        [ -9.0516,  -3.2974,   6.8151],\n","        [ -5.3327,  -2.6336,  13.2100],\n","        [  5.1068,  -3.9332, -15.8885],\n","        [ -1.4233,   6.5601,  -1.1260]], dtype=torch.float64,\n","       requires_grad=True)\n"]}]},{"cell_type":"code","source":["# The following does not work in PyTorch\n","#y = torch.tensor(['non-diabetic', 'diabetic'])\n","\n","# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n","# PyTorch which itself is equivalent to a vector in pen & paper)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","print(y)\n","print(type(y))\n","print(y.shape)\n","y = y.reshape(-1, 1)\n","print('------')\n","print(y)\n","print(type(y))\n","print(y.shape)\n","print('-------')\n","# Creating a one-hot encoder object\n","ohe = OneHotEncoder(sparse_output = False)\n","# Create the one-hot encoded true output labels matrix\n","Y = torch.tensor(ohe.fit_transform(y), dtype = torch.float64)\n","print(Y)"],"metadata":{"id":"7X-qxW-Dk2gT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756544301001,"user_tz":-330,"elapsed":57,"user":{"displayName":"rnvinay","userId":"12595551821676752839"}},"outputId":"a3de63a0-9e34-423f-8761-2f0e8d270764"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['non-diabetic' 'diabetic' 'non-diabetic' 'pre-diabetic' 'diabetic'\n"," 'pre-diabetic']\n","<class 'numpy.ndarray'>\n","(6,)\n","------\n","[['non-diabetic']\n"," ['diabetic']\n"," ['non-diabetic']\n"," ['pre-diabetic']\n"," ['diabetic']\n"," ['pre-diabetic']]\n","<class 'numpy.ndarray'>\n","(6, 1)\n","-------\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]], dtype=torch.float64)\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"colab-windows","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}